import SwiftUI
import Combine
@preconcurrency import MediaPipeTasksGenAI // Ensure this import matches your project setup

@MainActor
class LlmInferenceService: ObservableObject {

    private var llmInference: LlmInference?
    private let modelName: String
    private let modelExtension: String = "tflite"

    // Use your existing ChatMessage struct
    @Published var conversation: [ChatMessage] = []
    @Published var isLoading: Bool = false
    @Published var initErrorMessage: String? // For critical initialization errors

    private let maxTokensConfig: Int = 256
    private var currentStreamingTask: Task<Void, Never>?

    init(modelName: String = "gemma-2b-it-gpu-int8") { // Ensure this matches your model name
        self.modelName = modelName
        setupLlm(isInitialSetup: true)
    }

    private func setupLlm(isInitialSetup: Bool = false) {
        if !isInitialSetup && llmInference != nil {
            print("Deallocating previous LlmInference engine instance for re-initialization.")
            llmInference = nil
        } else if !isInitialSetup && llmInference == nil {
            print("LLM instance was already nil before re-initialization attempt.")
        }
        
        print(isInitialSetup ? "Attempting initial LLM setup..." : "Setting up LlmInference engine (maxTokens: \(self.maxTokensConfig))...")

        guard let modelPath = Bundle.main.path(forResource: self.modelName, ofType: self.modelExtension) else {
            let errorMsg = "CRITICAL ERROR: Failed to find model file: '\(self.modelName).\(self.modelExtension)'."
            print(errorMsg)
            self.initErrorMessage = errorMsg
            // Use your existing Sender type for error messages
            if isInitialSetup || self.conversation.filter({ $0.sender == .error(isCritical: true) }).isEmpty {
                self.conversation.append(ChatMessage(sender: .error(isCritical: true), text: errorMsg))
            }
            self.llmInference = nil
            return
        }
        if isInitialSetup { print("Model path found: \(modelPath)") }

        let options = LlmInference.Options(modelPath: modelPath)
        options.maxTokens = self.maxTokensConfig
        
        do {
            print("Initializing LlmInference instance with options (maxTokens: \(options.maxTokens))...")
            llmInference = try LlmInference(options: options)
            let successMsg = isInitialSetup ? "LlmInference initialized successfully." : "LlmInference (re-)initialized successfully."
            print(successMsg)
            self.initErrorMessage = nil
        } catch {
            let errorMsg = "Failed to initialize LlmInference: \(error.localizedDescription)."
            print(errorMsg)
            self.initErrorMessage = errorMsg
            // Use your existing Sender type for error messages
            if isInitialSetup || self.conversation.filter({ $0.sender == .error(isCritical: true) }).isEmpty {
                 self.conversation.append(ChatMessage(sender: .error(isCritical: true), text: errorMsg))
            }
            self.llmInference = nil
        }
    }
    
    func startNewChat() {
        stopGeneration()
        conversation.removeAll()
        isLoading = false
        initErrorMessage = nil
        print("New chat started. Forcing LLM re-setup for a clean slate.")
        setupLlm(isInitialSetup: false)
    }

    func generateResponseStreaming(prompt: String) {
        if let criticalError = self.initErrorMessage {
            print("Cannot generate response due to existing critical initialization error: \(criticalError)")
            // Use your existing Sender type
            if conversation.last?.text != criticalError && conversation.last?.sender != .error(isCritical: true) {
                self.conversation.append(ChatMessage(sender: .error(isCritical: true), text: criticalError))
            }
            return
        }

        guard currentStreamingTask == nil else {
            let errMsg = "A response is already being generated by the app. Please wait."
            print(errMsg)
            return
        }
        
        guard !self.isLoading else {
            print("Streaming request attempted while 'isLoading' is true (UI or state inconsistency).")
            return
        }

        print("Preparing for new stream. Re-initializing LLM engine proactively.")
        setupLlm(isInitialSetup: false)

        guard let llmForTask = self.llmInference else {
            let errMsg = self.initErrorMessage ?? "LLM Inference service became unavailable after re-setup attempt. Please try starting a new chat."
            print("LLM re-initialization failed or instance is nil before streaming.")
            // Use your existing Sender type
            if conversation.last?.text != errMsg {
                self.conversation.append(ChatMessage(sender: .error(), text: errMsg))
            }
            self.isLoading = false
            return
        }
        
        // Use your existing Sender type
        let userMessage = ChatMessage(sender: .user, text: prompt)
        self.conversation.append(userMessage)

        self.isLoading = true

        let systemInstruction = "You are Lumi, a friendly and concise human like assistant. Your primary user is Matt. If you don't know an answer do not make up an answer. Do no repeat back a query when answering."
        let finalPrompt = systemInstruction + prompt
        
        print("Generating streaming response for final prompt (first 80 chars): \(String(finalPrompt.prefix(80)))...")

        var lumiMessageIndex: Int? = nil
        // Use your existing Sender type
        let initialLumiMessage = ChatMessage(sender: .lumi, text: "")
        conversation.append(initialLumiMessage)
        lumiMessageIndex = conversation.count - 1

        currentStreamingTask = Task {
            var accumulatedResponseInTask = ""
            var taskError: Error? = nil
            var responseReceived = false

            do {
                let responseStream = llmForTask.generateResponseAsync(inputText: finalPrompt)
                for try await partialResult in responseStream {
                    if Task.isCancelled {
                        taskError = CancellationError()
                        print("Streaming task was cancelled.")
                        break
                    }
                    responseReceived = true
                    accumulatedResponseInTask += partialResult
                    let currentSnapshot = accumulatedResponseInTask
                    
                    await MainActor.run {
                        // Use your existing Sender type
                        if let idx = lumiMessageIndex, idx < self.conversation.count, self.conversation[idx].sender == .lumi {
                            self.conversation[idx].text = currentSnapshot
                        }
                    }
                }
                if taskError == nil && responseReceived {
                     print("Streaming finished successfully.")
                } else if taskError == nil && !responseReceived {
                    print("Stream ended without any response data.")
                }
            } catch {
                taskError = error
                print("Error during streaming on background task: \(error.localizedDescription)")
            }

            await MainActor.run {
                if let idx = lumiMessageIndex, idx < self.conversation.count {
                    // Use your existing Sender type
                    if self.conversation[idx].sender == .lumi {
                        if let error = taskError {
                            let nsError = error as NSError
                            var displayMessage: String
                            // Use your existing Sender type
                            var messageSenderType: Sender = .error() // Corrected variable name

                            if nsError.domain == NSURLErrorDomain && nsError.code == NSURLErrorCancelled || error is CancellationError {
                                displayMessage = "(Stopped by user)"
                                messageSenderType = .info // Corrected variable name
                                if self.conversation[idx].text.isEmpty { self.conversation.remove(at: idx); lumiMessageIndex = nil }
                                else { self.conversation[idx].text += "\n" + displayMessage }
                            } else {
                                displayMessage = "Error: \(error.localizedDescription)"
                                if self.conversation[idx].text.isEmpty {
                                     self.conversation[idx].text = displayMessage
                                     self.conversation[idx].sender = messageSenderType // Use corrected variable name
                                } else {
                                     self.conversation[idx].text += "\n\n\(displayMessage)"
                                }
                            }
                        } else if !responseReceived && self.conversation[idx].text.isEmpty {
                            // Use your existing Sender type
                            self.conversation[idx].sender = .info
                            self.conversation[idx].text = "(Lumi provided no response)"
                        }
                    }
                } else if let error = taskError, !(error is CancellationError) {
                    let errorMsg = "Streaming error (orphaned): \(error.localizedDescription)"
                    // Use your existing Sender type
                     if self.conversation.last?.text != errorMsg {
                        self.conversation.append(ChatMessage(sender: .error(), text: errorMsg))
                    }
                }

                self.isLoading = false
                self.currentStreamingTask = nil
                print("Stream processing finished. isLoading set to false, currentStreamingTask cleared.")
            }
        }
    }

    func stopGeneration() {
        if let task = currentStreamingTask {
            if !task.isCancelled {
                task.cancel()
                print("Stop generation requested. Task cancellation initiated.")
            } else {
                print("Stop generation requested, but task was already cancelled.")
            }
        } else {
            print("Stop generation requested, but no active streaming task found.")
            if isLoading {
                isLoading = false
                print("isLoading was true with no task, reset to false during stopGeneration.")
            }
        }
    }
}
